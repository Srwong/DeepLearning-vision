{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pong_leaky.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "tAX6dynh1ENW"
      },
      "cell_type": "markdown",
      "source": [
        "# Playing Pong with Deep Reinforcement Learning\n",
        "\n",
        "---\n",
        "\n",
        "Read the paper [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf) (the paper is also inside the 'Papers' folder in the course materials), and implement a model that can play atari games.\n",
        "\n",
        "The goals of this project are the following:\n",
        "\n",
        "- Read and understand the paper.\n",
        "- Add a brief summary of the paper at the start of the notebook.\n",
        "- Mention and implement the preprocessing needed; you can add your own steps if needed.\n",
        "- Load an Atari environment from OpenAI Gym; start with Pong, and try with at least one more.\n",
        "- Define the convolutional model needed for training.\n",
        "- Apply deep q learning with your model.\n",
        "- Use the model to play a game and show the result.\n",
        "\n",
        "**Rubric:**\n",
        "\n",
        "1. A summary of the paper was included. The summary covered what the paper does, and why, as well as the preprocessing steps and the model they introduced.\n",
        "2. Read images from the environment, and performed the correct preprocessing steps.\n",
        "3. Defined an agent class with the needed functions.\n",
        "4. Defined the model within the agent class.\n",
        "5. Trained the model with the Pong environment. Save the weights after each episode.\n",
        "6. Test the model by making it play Pong.\n",
        "7. Train and test the agent with another Atari environment of your choosing.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ahIZHdp12nY-"
      },
      "cell_type": "markdown",
      "source": [
        "## Add a summary of the paper in this cell"
      ]
    },
    {
      "metadata": {
        "id": "cV0S4zYRLEeH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The paper talked about the posibility to create a neuronal network that can learn to play different games without any changes in the architecture of the network, the only change was the input that was a part of the screen of the game the network tried to learn.\n",
        "\n",
        "They described how they calculated the reward and function implemented. Also mentioned that one movement in the game may take a lot of timesteps to see that movement result and give a proper reward, either negative or positive. There was a problem with the score too, different games means different ways to increase the score and therefore different increasing values, they opted to set positive rewars to one and negative to -1 to reduce complexity.\n",
        "\n",
        "The images were preprocessed by pasing the image to gray scale, downsampling and corping it. This results in a square image. Then they merged four of those images (differente frames) and sended to the network, so the input was NxNx4. Also there was an skipping of four frames to speed up the training and it didn't affected the trainning to much, just in space invaders where the laser blinks.\n",
        "\n",
        "The network they used was a small one, two convolutional layers, the flatten and two dense layers, one for analisis and the other for the result.\n",
        "\n",
        "For their experiments they used a batch size of 32, RMSProp and a linear replay memory of 1 million."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NrmWWhzE1Vg1"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic installs and imports for Colab"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cmTlRpCTiUeo",
        "outputId": "b47b53db-3107-4493-ee88-964917365f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gym[box2d] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting setuptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/b0/cc6b7ba28d5fb790cf0d5946df849233e32b8872b6baca10c9e002ff5b41/setuptools-41.0.0-py2.py3-none-any.whl (575kB)\n",
            "\u001b[K    100% |████████████████████████████████| 583kB 26.6MB/s \n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Found existing installation: setuptools 40.9.0\n",
            "    Uninstalling setuptools-40.9.0:\n",
            "      Successfully uninstalled setuptools-40.9.0\n",
            "Successfully installed setuptools-41.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SOAgdSpoi8Md",
        "outputId": "366b8c7b-f558-4b1d-8c66-f2b8831383b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random, math\n",
        "\n",
        "from keras import models, layers, optimizers\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import glob, io, base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "gymlogger.set_level(40) #error only\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JCXxMUco1aoh"
      },
      "cell_type": "markdown",
      "source": [
        "### Functions that wraps a video in colab"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xguRhza7jSRD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2HI_wN3OjVd2",
        "outputId": "eca6e387-757d-4043-fae5-a916ef8d87b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "C3jpZDbFjJu8",
        "outputId": "df87d95d-d21f-40f9-f419-d63a71de122e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Loads the pong environment\n",
        "env = wrap_env(gym.make('Pong-v0'))\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print(state_size, action_size)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "n_episodes = 1001"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "210 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R5T1o_x-BI-T",
        "colab_type": "code",
        "outputId": "832ba0bf-7c3c-4cdd-9224-5410127deba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Could not find video\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "h3Ww0L4w1hE4"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the Deep Q learning Agent"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nbtt8Frsjvm8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    \n",
        "    def __init__(self, state_size, action_size):\n",
        "      \n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        \n",
        "        self.memory = deque(maxlen=40000) #40k * 4 = 160k frames\n",
        "        \n",
        "        self.gamma = 0.95\n",
        "        self.alpha = 0.001\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.99997\n",
        "        self.epsilon_min = 0.01\n",
        "        \n",
        "        self.model = self._build_model()\n",
        "        \n",
        "\n",
        "    def _build_model(self):\n",
        "        \n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Conv2D(16, (8,8), strides=4, input_shape = (4,84,84), data_format=\"channels_first\"))\n",
        "        model.add(layers.LeakyReLU(alpha=0.2))\n",
        "        model.add(layers.Conv2D(32, (4,4), strides=2))\n",
        "        model.add(layers.LeakyReLU(alpha=0.2))\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(256, activation='relu'))\n",
        "        model.add(layers.Dense(self.action_size, activation='softmax')) #movements\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done, screen):\n",
        "        '''\n",
        "            state, action, reward at current time\n",
        "            next_state is the state that occurs after the state-action\n",
        "            done is if the episode ended\n",
        "        '''\n",
        "        self.memory.append((state, action, reward, next_state, done, screen))\n",
        "        \n",
        "    def action(self, screen):\n",
        "        \n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        \n",
        "        \n",
        "        prediction = self.model.predict(screen)\n",
        "        #print(\"prediction: \",prediction)\n",
        "        \n",
        "        return np.argmax(prediction[0])\n",
        "        \n",
        "    def train(self, batch_size):\n",
        "        \n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        \n",
        "        for state, action, reward, next_state, done, screen in batch:\n",
        "          \n",
        "            #screen = np.expand_dims(screen, axis=0)\n",
        "            #print(\"screen: \",screen.shape)\n",
        "            \n",
        "            target = reward\n",
        "            \n",
        "            if not done:\n",
        "                \n",
        "                target = (reward + self.gamma * \n",
        "                          np.amax(self.model.predict(screen)[0]))\n",
        "                \n",
        "            target_y = self.model.predict(screen)\n",
        "            #print(target_y,action)\n",
        "            \n",
        "            target_y[0][action] = target\n",
        "            \n",
        "            self.model.fit(screen, target_y, epochs=1, verbose=0)\n",
        "            \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    \n",
        "           \n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "        \n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jyAVZOyPjzTr",
        "outputId": "4f724c31-069c-4160-c11c-e777265da1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(state_size, action_size)\n",
        "agent.model.summary()\n",
        "agent.model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 16, 20, 20)        4112      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 16, 20, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 9, 32)          10272     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 9, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2016)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               516352    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 1542      \n",
            "=================================================================\n",
            "Total params: 532,278\n",
            "Trainable params: 532,278\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "94rKT1nj1nMj"
      },
      "cell_type": "markdown",
      "source": [
        "### Needed preprocessing steps"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "F0YR2iCYd4OV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocessFrame(image): #210,160\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  image = cv2.resize(image,(84,110)) #resize\n",
        "  image = image[18:102, :]  # crop the useless areas\n",
        "  \n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kjt6IsGAldu_",
        "colab_type": "code",
        "outputId": "b572561d-7a2b-401b-dbac-93b9d5f3eab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"#env.reset()\n",
        "env.step(2)\n",
        "img = env.render(mode='rgb_array') #get the screen\n",
        "print(preprocessFrame(img).shape)\n",
        "\n",
        "plt.imshow(preprocessFrame(img), cmap=\"gray\")\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#env.reset()\\nenv.step(2)\\nimg = env.render(mode=\\'rgb_array\\') #get the screen\\nprint(preprocessFrame(img).shape)\\n\\nplt.imshow(preprocessFrame(img), cmap=\"gray\")'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OVf_LPgl1u3n"
      },
      "cell_type": "markdown",
      "source": [
        "## Training with the environment images\n",
        "instead of perform an action every four frames keep the same for another three and change it in the fourth"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eHcl0-LAj38H",
        "outputId": "50dd5b18-3d76-4f91-8507-3455786fb536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "cell_type": "code",
      "source": [
        "agent.load('15_9900_pong_extended_leaky_2Alpha.hdf5')\n",
        "agent.epsilon=  0.02650281148520746\n",
        "\n",
        "batch_size = 32\n",
        "frames_pack = []\n",
        "try:\n",
        "    for e in range(10001): #epochs\n",
        "        \n",
        "        state = env.reset()\n",
        "        \n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        action = 0 #do nothing in context of pong\n",
        "        \n",
        "        while not done:\n",
        "            \n",
        "            screen = env.render(mode='rgb_array') #get the screen\n",
        "            screen = preprocessFrame(screen)\n",
        "            frames_pack.append(screen) #stack images \n",
        "            screen = np.expand_dims(screen, axis=2)\n",
        "            screen = np.expand_dims(screen, axis=0)\n",
        "            #print(\"test: \", screen.shape)\n",
        "        \n",
        "            \n",
        "            if len(frames_pack) == 4: \n",
        "              frames_pack = np.array(frames_pack)\n",
        "              #frames_pack = np.reshape(frames_pack, (84,84,4))\n",
        "              frames_pack = np.expand_dims(frames_pack, axis=0)\n",
        "              #print(frames_pack.shape)\n",
        "              \n",
        "              # Takes a random action from the action space of the environment\n",
        "              action = agent.action(frames_pack) #change action\n",
        "              next_state, reward, done, info = env.step(action)\n",
        "              # Define the reward for this problem\n",
        "              reward = reward if not done else -1\n",
        "              total_reward += reward\n",
        "  \n",
        "              agent.remember(state,action,reward,next_state,done,frames_pack)\n",
        "              state = next_state\n",
        "              frames_pack = []\n",
        "            else:\n",
        "              #repeat the action\n",
        "              next_state, reward, done, info = env.step(action)\n",
        "              reward = reward if not done else -1\n",
        "              total_reward += reward\n",
        "              state = next_state\n",
        "              \n",
        "            \n",
        "        \n",
        "        if e % 100 == 0:\n",
        "            agent.save('16_{:05d}_pong_extended_leaky_2Alpha'.format(e) + '.hdf5')\n",
        "            print(e, \" : \",agent.epsilon, \" : \",total_reward)\n",
        "        if len(agent.memory) > batch_size:\n",
        "          agent.train(batch_size)\n",
        "                \n",
        "        \n",
        "finally:\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  :  0.02650281148520746  :  -20.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "100  :  0.026423421005152575  :  -21.0\n",
            "200  :  0.026344268343199622  :  -21.0\n",
            "300  :  0.026265352786952766  :  -21.0\n",
            "400  :  0.026186673626150122  :  -21.0\n",
            "500  :  0.026108230152657466  :  -21.0\n",
            "600  :  0.026030021660461835  :  -21.0\n",
            "700  :  0.02595204744566518  :  -21.0\n",
            "800  :  0.025874306806477967  :  -21.0\n",
            "900  :  0.02579679904321292  :  -21.0\n",
            "1000  :  0.025719523458278725  :  -21.0\n",
            "1100  :  0.02564247935617373  :  -21.0\n",
            "1200  :  0.025565666043479693  :  -21.0\n",
            "1300  :  0.025489082828855546  :  -21.0\n",
            "1400  :  0.025412729023031166  :  -21.0\n",
            "1500  :  0.02533660393880118  :  -21.0\n",
            "1600  :  0.025260706891018746  :  -21.0\n",
            "1700  :  0.025185037196589426  :  -21.0\n",
            "1800  :  0.02510959417446504  :  -21.0\n",
            "1900  :  0.02503437714563748  :  -21.0\n",
            "2000  :  0.024959385433132694  :  -21.0\n",
            "2100  :  0.02488461836200452  :  -21.0\n",
            "2200  :  0.024810075259328594  :  -21.0\n",
            "2300  :  0.024735755454196404  :  -21.0\n",
            "2400  :  0.02466165827770913  :  -21.0\n",
            "2500  :  0.024587783062971668  :  -21.0\n",
            "2600  :  0.02451412914508662  :  -21.0\n",
            "2700  :  0.024440695861148352  :  -21.0\n",
            "2800  :  0.02436748255023697  :  -21.0\n",
            "2900  :  0.024294488553412444  :  -21.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yoWQOzwWkzXs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!rm *\n",
        "\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "25FqNG8u2dDa"
      },
      "cell_type": "markdown",
      "source": [
        "### Test your model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o1Ti62JUgiNm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#1_9900 : 0.7430407020422212  : 40k\n",
        "#2_9900  :  0.5521094848913968  :  -20.0\n",
        "#3_9900  :  0.4102398192578718  :  -21.0\n",
        "#4_9900  :  0.30482488330704127  :  -20.0\n",
        "#5_9800  :  0.22717781765931463  :  -21.0\n",
        "#6_9900  :  0.16880236512199645  :  -21.0\n",
        "#7_2900  :  0.15473705986001693  :  -21.0\n",
        "#8_9900  :  0.1149759335903355  :  -21.0\n",
        "#9_3600  :  0.10320540371981428  :  -21.0\n",
        "#10_4600  :  0.08990190606057706  :  -21.0\n",
        "#11_3400  :  0.08118395371003878  :  -21.0\n",
        "#12_8300  :  0.06328914808241247  :  -21.0\n",
        "#13_7500  :  0.04802298438354698  :  -21.0\n",
        "#14_9900  :  0.03568303203051324  :  -21.0\n",
        "#15_9900  :  0.02650281148520746  :  -21.0\n",
        "print(agent.epsilon)\n",
        "#agent.load('12_9900_pong_extended_leaky_2Alpha.hdf5')\n",
        "env = wrap_env(gym.make('Pong-v0'))\n",
        "frames_pack= []\n",
        "\n",
        "try:\n",
        "      action = 0 #do nothing in context of pong\n",
        "\n",
        "      state = env.reset()\n",
        "\n",
        "      total_reward = 0\n",
        "      done = False\n",
        "      screen = env.render(mode='rgb_array') #get the screen\n",
        "      screen = preprocessFrame(screen)\n",
        "      screen = np.expand_dims(screen, axis=2)\n",
        "      screen = np.expand_dims(screen, axis=0)\n",
        "\n",
        "      while not done:\n",
        "      #for time in range(200):\n",
        "\n",
        "          #env.render()\n",
        "\n",
        "          # Takes a random action from the action space of the environment\n",
        "          screen = env.render(mode='rgb_array') #get the screen\n",
        "          screen = preprocessFrame(screen)\n",
        "          frames_pack.append(screen) #stack images \n",
        "          screen = np.expand_dims(screen, axis=2)\n",
        "          screen = np.expand_dims(screen, axis=0)\n",
        "          #action = agent.action(screen)\n",
        "          \n",
        "          if len(frames_pack) == 4:\n",
        "            \n",
        "            frames_pack = np.array(frames_pack)\n",
        "            frames_pack = np.expand_dims(frames_pack, axis=0)\n",
        "\n",
        "\n",
        "            # Takes a random action from the action space of the environment\n",
        "            action = agent.action(frames_pack)\n",
        "            \n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            # Define the reward for this problem\n",
        "            reward = reward if not done else -1\n",
        "\n",
        "            total_reward += reward\n",
        "            #print(total_reward)\n",
        "\n",
        "\n",
        "            state = next_state\n",
        "            frames_pack = []\n",
        "          else:\n",
        "            #repeat the action\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            reward = reward if not done else -1\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        \n",
        "finally:\n",
        "    env.close()       \n",
        "    show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ca3zSLF02x1E"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and test your agent with another atari environment"
      ]
    },
    {
      "metadata": {
        "id": "tF6zg3meqR8f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eps = 1.0\n",
        "decay = 0.99995\n",
        "count = 0\n",
        "\n",
        "while eps > 0.01:\n",
        "    count += 1\n",
        "    eps *= decay\n",
        "    if count % 100 == 0:\n",
        "      print(count,\" : \",eps)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}